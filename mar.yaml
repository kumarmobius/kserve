name: GPTNeoX MAR
description: |
  Creates a TorchServe .mar using handler.py, model.pth, and tokenizer.json.
  Generates model.py (+ model_config.json) via nesy_factory.language_model.gptneox.GPTNeoXBuilder.
  Stages config/config.properties next to the MAR for KServe.
inputs:
  - {name: handler_file, type: String, description: "Directory containing handler.py"}
  - {name: tokenizer_json, type: Model, description: "Path to tokenizer.json"}
  - {name: model_pth_file, type: Model, description: "Path to trained weights (e.g., model.pth / learned_weights.pth)"}
  - {name: config_properties, type: Data, description: "Path to config.properties"}
  - {name: model_name, type: String, default: "gptneox", description: "Name for the MAR"}
  - {name: model_version, type: String, default: "1.0", description: "Version for the MAR"}
outputs:
  - {name: mar_file_out, type: String, description: "Output directory containing {model_name}.mar and config/"}
  - {name: mar_creation_log, type: String, description: "JSON log with details"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install -q torch-model-archiver || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install -q torch-model-archiver --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import shutil
        import argparse
        import subprocess
        from datetime import datetime

        def ensure_exists(path, label):
            if not os.path.exists(path):
                raise FileNotFoundError(f"{label} not found: {path}")
            return path

        def stage_file(src_path, dst_name):
            dst_path = os.path.join(os.getcwd(), dst_name)
            shutil.copy2(src_path, dst_path)
            return dst_path

        def generate_model_files(tokenizer_path):
            # Use nesy_factory builder to produce model.py and model_config.json
            from nesy_factory.language_model.gptneox import GPTNeoXBuilder
            builder = GPTNeoXBuilder({})
            generated_weights = os.path.abspath("discard_weights.pth")
            generated_cfg = os.path.abspath("model_config.json")
            out_py = os.path.abspath("model.py")
            res = builder.run(
                tokenizer_json=os.path.abspath(tokenizer_path),
                n_layers=12,
                layer_pattern="",
                model_weights_out=generated_weights,
                model_config_out=generated_cfg,
                model_py_out=out_py,
            )
            return out_py, generated_cfg

        def create_mar(model_name, model_version, model_py, handler_py, weights_path, extra_files, out_dir):
            os.makedirs(out_dir, exist_ok=True)
            cmd = [
                "torch-model-archiver",
                "--model-name", model_name,
                "--version", model_version,
                "--model-file", model_py,
                "--serialized-file", weights_path,
                "--handler", handler_py,
                "--export-path", out_dir,
                "--force",
            ]
            if extra_files:
                cmd += ["--extra-files", ",".join(extra_files)]
            print("MAR command:")
            print("  " + " ".join(cmd))
            res = subprocess.run(cmd, capture_output=True, text=True)
            if res.returncode != 0:
                print("MAR creation failed")
                print("STDOUT-->")
                print(res.stdout)
                print("STDERR-->")
                print(res.stderr)
                raise RuntimeError("torch-model-archiver failed")
            mar_path = os.path.join(out_dir, f"{model_name}.mar")
            if not os.path.exists(mar_path):
                raise RuntimeError("MAR not found at expected path: " + mar_path)
            print("MAR created at:", mar_path)
            return mar_path, res.stdout

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--handler_file", required=True)       # dir containing handler.py
            ap.add_argument("--tokenizer_json", required=True)     # file
            ap.add_argument("--model_pth_file", required=True)     # file
            ap.add_argument("--config_properties", required=True)  # file
            ap.add_argument("--model_name", default="gptneox")
            ap.add_argument("--model_version", default="1.0")
            ap.add_argument("--mar_file_out", required=True)
            ap.add_argument("--mar_creation_log", required=True)
            args = ap.parse_args()

            log = {
                "timestamp": datetime.now().isoformat(),
                "model_name": args.model_name,
                "model_version": args.model_version,
                "status": "starting",
                "steps": []
            }

            try:
                handler_py = ensure_exists(os.path.join(args.handler_file, "handler.py"), "handler.py")
                tokenizer_path = ensure_exists(args.tokenizer_json, "tokenizer.json")
                weights_path = ensure_exists(args.model_pth_file, "model.pth/.pth")
                cfg_props_path = ensure_exists(args.config_properties, "config.properties")
                print("Inputs OK")
                log["steps"].append({"step": 1, "action": "validate_inputs", "status": "success"})

                # Stage tokenizer.json inside CWD with predictable name
                staged_tok = stage_file(tokenizer_path, "tokenizer.json")
                print("Staged tokenizer:", staged_tok)

                # Generate model.py and model_config.json via builder
                model_py, model_cfg = generate_model_files(staged_tok)
                print("Generated model.py:", model_py)
                print("Generated model_config.json:", model_cfg)
                extras = [staged_tok, model_cfg]
                log["steps"].append({"step": 2, "action": "generate_model_files", "status": "success", "extras": extras})

                # Create MAR
                out_dir = args.mar_file_out
                mar_path, stdout_text = create_mar(args.model_name, args.model_version, model_py, handler_py, weights_path, extras, out_dir)
                log["steps"].append({"step": 3, "action": "create_mar", "status": "success", "mar_path": mar_path})

                # Stage config/config.properties NEXT TO the MAR (not inside)
                cfg_dir = os.path.join(out_dir, "config")
                os.makedirs(cfg_dir, exist_ok=True)
                shutil.copy2(cfg_props_path, os.path.join(cfg_dir, "config.properties"))
                print("Staged config/config.properties")
                log["steps"].append({"step": 4, "action": "stage_config_properties", "status": "success", "config_dir": cfg_dir})

                log["status"] = "completed"
                log["mar_file_path"] = mar_path
                log["note"] = "Upload mar_file_out/ to MinIO root. It contains model-store/{name}.mar and config/config.properties."

            except Exception as e:
                log["status"] = "failed"
                log["error"] = str(e)
                print("ERROR:", str(e), file=sys.stderr)
                sys.exit(1)
            finally:
                os.makedirs(os.path.dirname(args.mar_creation_log) or ".", exist_ok=True)
                with open(args.mar_creation_log, "w", encoding="utf-8") as f:
                    json.dump(log, f, indent=2)
                print("Log saved:", args.mar_creation_log)

        if __name__ == "__main__":
            main()
    args:
      - --handler_file
      - {inputPath: handler_file}
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --model_pth_file
      - {inputPath: model_pth_file}
      - --config_properties
      - {inputPath: config_properties}
      - --model_name
      - {inputValue: model_name}
      - --model_version
      - {inputValue: model_version}
      - --mar_file_out
      - {outputPath: mar_file_out}
      - --mar_creation_log
      - {outputPath: mar_creation_log}
