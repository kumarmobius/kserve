name: GPTNeoX MAR File Creator
description: |
  Creates a TorchServe .mar using handler.py, model.pth, tokenizer.json. 
  Generates model.py (+ model_config.json) via nesy_factory.language_model.gptneox.GPTNeoXBuilder.
  Also stages config/config.properties beside the MAR for KServe.
inputs:
  - {name: handler_file, type: String, description: "Directory that contains handler.py"}
  - {name: tokenizer_json, type: Model, description: "Path to tokenizer.json"}
  - {name: model_pth_file, type: Model, description: "Path to trained weights (e.g., model.pth / learned_weights.pth)"}
  - {name: config_properties, type: Data, description: "Path to config.properties (TorchServe server config)"}
  - {name: model_name, type: String, default: "gptneox", description: "MAR name (and InferenceService model id)"}
  - {name: model_version, type: String, default: "1.0", description: "MAR version"}
outputs:
  - {name: mar_file_out, type: String, description: "Output directory containing {model_name}.mar and a config/ subdir"}
  - {name: mar_creation_log, type: String, description: "JSON log with details"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch-model-archiver || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet torch-model-archiver --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, sys, json, shutil, argparse, subprocess
        from datetime import datetime

        def must(path, label):
            if not os.path.exists(path):
                raise FileNotFoundError(f"{label} not found: {path}")
            return path

        def stage(src, dst_name):
            """Copy a file into CWD with a fixed name (predictable inside MAR)."""
            if not src: return None
            must(src, "File")
            dst = os.path.join(os.getcwd(), dst_name)
            shutil.copy2(src, dst)
            return dst

        def create_model_py_and_config(tokenizer_path):
            """
            Use nesy_factory GPTNeoXBuilder to generate model.py and model_config.json.
            We IGNORE the builder's generated weights and use the user's trained weights file.
            """
            from nesy_factory.language_model.gptneox import GPTNeoXBuilder
            builder = GPTNeoXBuilder({})
            gen_weights = os.path.abspath("discard_weights.pth")   # placeholder; will not be packaged
            gen_cfg = os.path.abspath("model_config.json")
            out_py = os.path.abspath("model.py")
            res = builder.run(
                tokenizer_json=os.path.abspath(tokenizer_path),
                n_layers=12,
                layer_pattern="",
                model_weights_out=gen_weights,
                model_config_out=gen_cfg,
                model_py_out=out_py,
            )
            return out_py, gen_cfg

        def create_mar(model_name, model_version, model_file, handler_path, weights_path, extras, out_dir):
            os.makedirs(out_dir, exist_ok=True)
            cmd = [
                "torch-model-archiver",
                "--model-name", model_name,
                "--version", model_version,
                "--model-file", model_file,
                "--serialized-file", weights_path,
                "--handler", handler_path,
                "--export-path", out_dir,
                "--force",
            ]
            if extras:
                cmd += ["--extra-files", ",".join(extras)]
            print("MAR command:\n  " + " ".join(cmd))
            res = subprocess.run(cmd, capture_output=True, text=True)
            if res.returncode != 0:
                print("STDOUT:\n", res.stdout)
                print("STDERR:\n", res.stderr)
                raise RuntimeError("torch-model-archiver failed")
            mar_path = os.path.join(out_dir, f"{model_name}.mar")
            if not os.path.exists(mar_path):
                raise RuntimeError("MAR not found: " + mar_path)
            return mar_path, res.stdout

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--handler_file", required=True)       # dir
            ap.add_argument("--tokenizer_json", required=True)     # file
            ap.add_argument("--model_pth_file", required=True)     # file
            ap.add_argument("--config_properties", required=True)  # file
            ap.add_argument("--model_name", default="gptneox")
            ap.add_argument("--model_version", default="1.0")
            ap.add_argument("--mar_file_out", required=True)
            ap.add_argument("--mar_creation_log", required=True)
            args = ap.parse_args()

            log = {
              "timestamp": datetime.now().isoformat(),
              "model_name": args.model_name,
              "model_version": args.model_version,
              "status": "starting",
              "steps": []
            }

            try:
              # 1) Validate inputs
              handler_py = must(os.path.join(args.handler_file, "handler.py"), "handler.py")
              tok_json = must(args.tokenizer_json, "tokenizer.json")
              weights = must(args.model_pth_file, "model.pth")
              cfg_props = must(args.config_properties, "config.properties")
              print(f"Inputs OK\n  handler: {handler_py}\n  weights: {weights}\n  tokenizer: {tok_json}\n  config.properties: {cfg_props}")
              log["steps"].append({"step":1,"action":"validate_inputs","status":"success"})

              # 2) Stage tokenizer.json so name is predictable inside MAR
              staged_tok = stage(tok_json, "tokenizer.json")
              print(f"Staged tokenizer: {staged_tok}")

              # 3) Generate model.py and model_config.json from nesy_factory builder
              model_py, model_cfg = create_model_py_and_config(staged_tok)
              print(f"Generated model.py: {model_py}\nGenerated model_config.json: {model_cfg}")
              extras = [staged_tok, model_cfg]
              log["steps"].append({"step":2,"action":"generate_model_files","status":"success","model_py":model_py,"extras":extras})

              # 4) Create MAR (handler + weights + tokenizer.json + model_config.json)
              out_dir = args.mar_file_out
              mar_path, stdout = create_mar(args.model_name, args.model_version, model_py, handler_py, weights, extras, out_dir)
              print("MAR created at:", mar_path)
              log["steps"].append({"step":3,"action":"create_mar","status":"success","mar_path":mar_path})

              # 5) Copy config.properties into output/config/ (NOT inside the MAR)
              cfg_dir = os.path.join(out_dir, "config")
              os.makedirs(cfg_dir, exist_ok=True)
              shutil.copy2(cfg_props, os.path.join(cfg_dir, "config.properties"))
              print("Staged config/config.properties next to MAR.")
              log["steps"].append({"step":4,"action":"stage_config_properties","status":"success","config_dir":cfg_dir})

              log["status"] = "completed"
              log["mar_file_path"] = mar_path
              log["note"] = "Upload mar_file_out/ to MinIO root; it already contains model-store/{model_name}.mar and config/config.properties"

            except Exception as e:
              log["status"] = "failed"
              log["error"] = str(e)
              print("ERROR:", e, file=sys.stderr)
              sys.exit(1)
            finally:
              os.makedirs(os.path.dirname(args.mar_creation_log) or ".", exist_ok=True)
              with open(args.mar_creation_log, "w", encoding="utf-8") as f:
                json.dump(log, f, indent=2)
              print("Log saved:", args.mar_creation_log)

        if __name__ == "__main__":
            main()
    args:
      - --handler_file
      - {inputPath: handler_file}
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --model_pth_file
      - {inputPath: model_pth_file}
      - --config_properties
      - {inputPath: config_properties}
      - --model_name
      - {inputValue: model_name}
      - --model_version
      - {inputValue: model_version}
      - --mar_file_out
      - {outputPath: mar_file_out}
      - --mar_creation_log
      - {outputPath: mar_creation_log}
